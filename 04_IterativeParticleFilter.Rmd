---
title: "04_IterativeParticleFilter"
author: "Nia Bartolucci"
date: "4/26/2021"
output: html_document
---

Potential issues with this iterative forecast:
1. Initial conditions/model parameters sampled independently from the chain rather than jointly. I think it's better to sample them jointly (with replacement) because they could be correlated under the chain.
2. Standard deviation for the particle filter (Chunk 9) is currently just 1. This should be obtained by the data as suggested by Kangjoon.
3. Due to issues with the numbers, some questionable math is used. We just normalized the weights so they all lie between 10^-20, 10^20. Maybe one should resample instead.
4. This is not really a problem with the forecast, but rather with the model. The model thinks the NEE is periodic with a similar amplitude year round, which makes it very inaccurate in the winter.





```{r}
## Package check and load

#install.packages("tidyverse")
#install.packages("readr")
library(tidyverse)
library(readr)
library(rjags)
library(rnoaa)
library(daymetr)
#library(ecoforecastR)
library(dplyr)
library(iotools)
source("/Users/niabartolucci/Dropbox/My Mac (Niaâ€™s MacBook Pro)/Desktop/Classes Spring 2021/Ecological Forecasting/EF_Activities/ecoforecastR/R/utils.R")

#remotes::install_github("EcoForecast/ecoforecastR",force=TRUE)
```

```{r}
# If you need run data download
###source('01A_Targetdownload.R')

# definition for PATH
basePath <- getwd() 
graphPath <- paste0(basePath,"/graph/")
dataPath <- paste0(basePath,"/data/")

# load the data file [30 min Target data]
loadFilename <- sprintf("%s.Rdata","Target_30min")
loadFilename <- paste(dataPath, loadFilename, sep="", collapse = NULL)
load(file = loadFilename)

Target_30min_KONZ = subset(Target_30min, siteID == 'KONZ' & time >= as.POSIXct('2020-03-01 00:00', tz="UTC") & 
                                                                time < as.POSIXct('2021-03-01 00:00', tz="UTC"))
time_KONZ = Target_30min_KONZ$time
```

```{r}
site_names <- c("BART","KONZ","OSBS","SRER")

#NOAA data load
# Download NOAA climate forecasts (hourly) and downsample to daily scale
source("00B_NOAAconversion.R")
for (S in site_names){
  download_noaa_files_s3(siteID = S,
                         date = "2021-03-01",
                         cycle = "00",
                         local_directory <- paste0(basePath,"/drivers/"))
}
NOAA_Driver = noaa_gefs_read(paste0(basePath,"/drivers/noaa/NOAAGEFS_1hr"), "2021-03-01", "00", "KONZ")
predict_time = subset(NOAA_Driver, ensemble==1)
predict_time = predict_time$time
## Driver data conversion
sw_driver = subset(NOAA_Driver, ensemble!=0)
sw_driver = sw_driver$surface_downwelling_shortwave_flux_in_air
sw_driver = matrix(sw_driver, nrow=30 ,byrow = TRUE)
lw_driver = subset(NOAA_Driver, ensemble!=0)
lw_driver = lw_driver$surface_downwelling_longwave_flux_in_air
lw_driver = matrix(lw_driver, nrow=30 ,byrow = TRUE)
temp_driver = subset(NOAA_Driver, ensemble!=0)
temp_driver = temp_driver$air_temperature
temp_driver = matrix(temp_driver, nrow=30 ,byrow = TRUE)
tmp = matrix(273.15,30,841)
temp_driver = temp_driver - tmp  # conversion (-273.15)
precip_driver = subset(NOAA_Driver, ensemble!=0)
precip_driver = precip_driver$precipitation_flux
precip_driver = matrix(precip_driver, nrow=30 ,byrow = TRUE)
precip_driver = 1800 * precip_driver # unit conversion (30 min -> 1800 sec)
sw_driver_gf = matrix(0, nrow=30, ncol=1681)
lw_driver_gf = matrix(0, nrow=30, ncol=1681)
temp_driver_gf = matrix(0, nrow=30, ncol=1681)
precip_driver_gf = matrix(0, nrow=30, ncol=1681)
## filling gap (average)
for(i in 1:840){
  sw_driver_gf[,2*i-1]=sw_driver[,i]
  sw_driver_gf[,2*i]=(sw_driver[,i]+sw_driver[,i+1])/2
  lw_driver_gf[,2*i-1]=lw_driver[,i]
  lw_driver_gf[,2*i]=(lw_driver[,i]+lw_driver[,i+1])/2
  temp_driver_gf[,2*i-1]=temp_driver[,i]
  temp_driver_gf[,2*i]=(temp_driver[,i]+temp_driver[,i+1])/2
  precip_driver_gf[,2*i-1]=precip_driver[,i]
  precip_driver_gf[,2*i]=(precip_driver[,i]+precip_driver[,i+1])/2
}
sw_driver_gf[,1681]=sw_driver[,841]
lw_driver_gf[,1681]=lw_driver[,841]
temp_driver_gf[,1681]=temp_driver[,841]
precip_driver_gf[,1681]=precip_driver[,841]
```

```{r}
#SET ENSEMBLE RUNS
ne = 1000         #needs to stay 30 unless we also sample (with replacement) the noaa driver ensembles
#yes, we will want way more than 30, so will have to sample with replacement

re_sw_driver_gf=sw_driver_gf[sample(nrow(sw_driver_gf),size=ne,replace=TRUE),]
re_lw_driver_gf=lw_driver_gf[sample(nrow(sw_driver_gf),size=ne,replace=TRUE),]
re_temp_driver_gf=temp_driver_gf[sample(nrow(sw_driver_gf),size=ne,replace=TRUE),]
re_precip_driver_gf=precip_driver_gf[sample(nrow(sw_driver_gf),size=ne,replace=TRUE),]

#AVERAGE CHAINS

#parsing MCMC output 
beta_LE = sample(joint_out$params[[2]][,1], ne, replace=TRUE)
beta_LEI = sample(joint_out$params[[2]][,2], ne, replace = TRUE)
beta_LN = sample(joint_out$params[[2]][,3], ne, replace = TRUE)
beta_LV = sample(joint_out$params[[2]][,4], ne, replace = TRUE)
beta_NEE = sample(joint_out$params[[2]][,5], ne, replace = TRUE)
beta_NEEI = sample(joint_out$params[[2]][,6], ne, replace = TRUE)
beta_NL = sample(joint_out$params[[2]][,7], ne, replace = TRUE)
beta_NV = sample(joint_out$params[[2]][,8], ne, replace = TRUE)
beta_VL = sample(joint_out$params[[2]][,9], ne, replace = TRUE)
beta_VN = sample(joint_out$params[[2]][,10], ne, replace = TRUE)
beta_VSWC = sample(joint_out$params[[2]][,11], ne, replace = TRUE)
beta_VSWCI = sample(joint_out$params[[2]][,12], ne, replace = TRUE)
beta_lw = sample(joint_out$params[[2]][,13], ne, replace = TRUE)
beta_precip = sample(joint_out$params[[2]][,14], ne, replace = TRUE)
beta_sw1 = sample(joint_out$params[[2]][,15], ne, replace = TRUE)
beta_sw2 = sample(joint_out$params[[2]][,16], ne, replace = TRUE)
beta_temp = sample(joint_out$params[[2]][,17], ne, replace = TRUE)
tau_le_add = sample(joint_out$params[[2]][,18], ne, replace = TRUE)
tau_le_obs = sample(joint_out$params[[2]][,19], ne, replace = TRUE)
tau_nee_add = sample(joint_out$params[[2]][,20], ne, replace = TRUE)
tau_nee_obs = sample(joint_out$params[[2]][,21], ne, replace = TRUE)
tau_vswc_add = sample(joint_out$params[[2]][,22], ne, replace = TRUE)
tau_vswc_obs = sample(joint_out$params[[2]][,23], ne, replace = TRUE)
#why sample from chain 2? 

#I feel that sampling from each of these independently is not correct, rather we should sample 30 random
#iterates of the 500*3 different iterates from the chains. 
#For now we can leave it this way.


#Initial conditions: starting from last observed value <-- is this a bad idea? -- yes :P
#qa_nee = joint_out$data$NEE_obs[!is.na(joint_out$data$NEE_obs)]
#IC_NEE = rnorm(ne, mean = qa_nee[length(qa_nee)], sd = 0.1)
#rm(qa_nee)


#qa_le = joint_out$data$LE_obs[!is.na(joint_out$data$LE_obs)]
#IC_LE = rnorm(ne, mean = qa_le[length(qa_le)], sd = 0.1)
#rm(qa_le)

#qa_vswc = joint_out$data$VSWC_obs[!is.na(joint_out$data$VSWC_obs)]   #remember outlier 
#IC_VSWC = rnorm(ne, mean = qa_vswc[length(qa_vswc)], sd = 0.1)
#rm(qa_vswc)

#the above seems to start at some arbitrary value, and add normal noise. 
#Instead we should sample from the chain output. 
#also arbitrarily sample form the second chain; could improve by sampling from all three.
#17520, 35040, 52560 are the last predictions for LE, NEE, VSWC repectively.

IC_LE = sample(joint_out$predict[[2]][,17520], ne, replace = TRUE)
IC_NEE = sample(joint_out$predict[[2]][,35040], ne, replace = TRUE)
IC_VSWC = sample(joint_out$predict[[2]][,52560], ne, replace = TRUE)






```

```{r}
ensembleforecast <- function(IC_NEE,IC_LE,IC_VSWC,
                     beta_NEE,beta_LE,beta_VSWC,beta_NL,beta_NV,beta_LV,beta_LN,
                     beta_VN,beta_VL,beta_NEEI,beta_LEI,beta_VSWCI,
                     beta_sw1,beta_sw2,beta_lw,beta_temp,beta_precip,
                     sw,lw,temp,precip){
  
  
  Nprev_NEE <- IC_NEE           
  Nprev_LE <- IC_LE
  Nprev_VSWC <- IC_VSWC
  NEE = (1+beta_NEE)*Nprev_NEE+beta_NEEI+beta_NL*Nprev_LE+beta_NV*Nprev_VSWC+beta_sw1*sw +beta_temp*temp
  LE = (1+beta_LE)*Nprev_LE+beta_LEI+beta_LN*Nprev_NEE+beta_LV*Nprev_VSWC+beta_sw2*sw +beta_lw*lw
  VSWC = (1+beta_VSWC)*Nprev_VSWC+beta_VSWCI+beta_VN*Nprev_NEE+beta_VL*Nprev_LE+beta_precip*precip
  return(cbind(NEE=NEE, LE=LE, VSWC=VSWC))
                     }
```

```{r}
#Initial Forecast

nt = 1681
#nt = 35 * 48                           ## 35 days of 30min; production run should be nrow(inputs) *********
output = array(0.0, c(ne, nt, 3))     ## output storage [time step,ensembles,variables]

## forward ensemble simulation
for(t in 1:nt){
  output[,t , ] <- ensembleforecast(IC_NEE,IC_LE,IC_VSWC,
                     beta_NEE,beta_LE,beta_VSWC,beta_NL,beta_NV,beta_LV,beta_LN,
                     beta_VN,beta_VL,beta_NEEI,beta_LEI,beta_VSWCI,
                     beta_sw1,beta_sw2,beta_lw,beta_temp,beta_precip,
                     re_sw_driver_gf[,t],re_lw_driver_gf[,t],re_temp_driver_gf[,t],re_precip_driver_gf[,t])  
  #reset initial conditions
  IC_NEE = output[,t ,1]
  IC_LE = output[,t ,2]
  IC_VSWC = output[,t ,3]
  #X <- output[t, , 1:3]                          ## set most recent prediction to be the next IC
  #if((t %% 336) == 0) print(t / 336)             ## counter: weeks elapsed (7*48 = 1 week)
}
```

```{r}
#Ensemble Forecast Visualizations
output[is.nan(output)] = 0
output[is.infinite(output)] = 0
output.ensemble = output                         ## save original ensemble projection

varnames = c("NEE", "LE", "VSWC")
units = c("units", "units", "units")            #Change later 
for(i in 1:3){  ## loop over variables
  ci = apply(output[, , i], 2, quantile, c(0.025, 0.5, 0.975))   ## calculate CI over ensemble members
  plot(ci[2, ], main = varnames[i], 
       xlab = "time", ylab = units[i], type='l',ylim  =range(ci))
  ciEnvelope(1:ncol(ci), ci[1, ], ci[3, ], col = col.alpha("lightGrey", 0.5)) ## plot interval
  lines(ci[2, ])                                                              ## plot median
}
```

```{r}
#get new data into the right format
NEEnew = data_up$NEE_obs

```



```{r}
## calculate the cumulative likelihoods
## to be used as PF weights

NEEcast = output[,,1]
NEElike = array(NA, dim(NEEcast))  ## storage, same size as model [ensemble, NEE new time points]
#NEEcalc = matrix(NA, nrow = nrow(NEEcast), ncol =length(NEEnew))
#print(dim(NEEcalc))
sel = 1:length(NEEnew)
for(i in 1:ne){
  #print(length(NEEcast[i, sel]))
  #print(length(NEEnew))
  NEElike[i, sel] = dnorm(NEEcast[i, sel], NEEnew, 1, log = TRUE)  ## calculate log likelihoods 
  
  #for now just using standard dev 1 from lack of standard dev data!! Need to fix this.
  
  NEElike[i, is.na(NEElike[i, ])] = 0       ## missing data as weight 1; log(1)=0
  #print(NEElike)
  #NEElike[i, ] = exp(cumsum(NEElike[i, ]))  ## convert to cumulative log likelihood and take out of log-space
  NEElike[i, ] = cumsum(NEElike[i, ])
  #set each column to have sum zero.
  #print(NEElike[i,1:500])
  #problem: there are way too many timepoints, so the exponentials of cumulative sums of likelihoods will be incredibly tiny. Need a more reasonable re-weighting method to make the quantiles for the particle filter. 
}
#set each column to have sum zero
NEEsum = colSums(NEElike)
NEEshift = array(1,dim(NEElike))
NEEshift = NEEshift%*%diag(NEEsum)
NEEshift = NEEshift/ne
NEElike = NEElike - NEEshift


#make sure the max and min are not too large

#NEEmax = apply(NEElike,2,max)
NEEmax = max(NEElike)
NEElike = 30*NEElike/NEEmax


#take out of log space
NEElike = exp(NEElike)

hist(NEElike[,ncol(NEElike)],main="Final Ensemble Weights", xlab = "NEElike for the ensemble")
#not showing anything because it is all zero.
``` 
Plotting the non-resampling particle filter

```{r}
## Non-resampling Particle Filter
## calculation of CI
#nobs = ncol(LAIlike)                     ## number of observations
NEEpf = matrix(NA, 3, nt)              ## storage [intervals,time]
wbar = apply(NEElike, 2, mean)           ## mean weight at each time point
for(i in 1:nt){
  ## calculate weighted median and CI
  vec1 = NEEcast[,i]
  wt = NEElike[, i] / wbar[i]
  NEEpf[, i] = wtd.quantile(NEEcast[, i], NEElike[, i] / wbar[i], c(0.025, 0.5, 0.975))  
}

## plot original ensemble and PF with data
col.pf   = c(col.alpha("lightGrey", 0.5), col.alpha("lightBlue", 0.5), 
             col.alpha("lightGreen", 0.5))                      ## color sequence
names.pf = c("ensemble", "non-resamp PF", "resamp PF")          ## legend names

ci = apply(output[, , 1], 2, quantile, c(0.025, 0.5, 0.975))   ## calculate CI over ensemble members
plot(ci[2, ], main = varnames[1], xlab = "time", ylab = units[1], type='l',ylim  =range(ci))
ciEnvelope(1:ncol(ci), ci[1, ], ci[3, ], col = col.alpha("lightGrey", 0.5)) ## plot interval
lines(ci[2, ])


ciEnvelope(1:ncol(ci), NEEpf[1, ], NEEpf[3, ], col = col.pf[2])      ## non-resampling Particle Filter
lines(NEEpf[2, ],col="green")

lines(NEEnew,col="red")
#points(Mtime, LAIr)                                                   ## observations
#for(i in 1:length(LAIr)){                                             ## observation uncertainty
#  if(!is.na(QC[i])){
#    lines(rep(Mtime[i], 2), LAIr[i]+c(-1, 1) * LAIr.sd[i])            ## data is +/- 1 SD; NOT 95%
#  }
#}
legend("topleft", legend = names.pf[1:2], col = col.pf[1:2], lwd = 5)
```
Can do this for the other variables.


Trying to plot the old stuff to compare
```{r}
#17520,35040
t1 = 3000
t2 = 10000
NEEpred = joint_out$predict[[2]][,(17521 + t1):(17521+ t2)]

ciNEE = apply(NEEpred, 2, quantile, c(0.025, 0.5, 0.975))
plot(ciNEE[2, ], xlab = "time", ylab = "NEE", type='l',ylim  =range(ciNEE))
ciEnvelope(1:ncol(ciNEE), ciNEE[1, ], ciNEE[3, ], col = col.alpha("lightGrey", 0.5)) ## plot interval
lines(ciNEE[2, ])
lines(NEE_KONZ[t1:t2],col="red")

```

```{r}
length(NEE_KONZ[t1:t2])

```